{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import evaluate\n",
    "import librosa\n",
    "import math\n",
    "import mir_eval\n",
    "import IPython.display as ipd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import medfilt as medfilt\n",
    "from scipy.misc import electrocardiogram\n",
    "from scipy import signal\n",
    "from scipy.ndimage import maximum_filter1d as maxfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#odf generators\n",
    "threshold = np.asarray([[0.1], [0.125], [0.2], [0.25], [0.0], [0.0], [0.0]])\n",
    "winSize_secs = 0.04\n",
    "hopSize_secs = 0.01\n",
    "wd=3\n",
    "selected_odf = 1\n",
    "\n",
    "dataDir = './BallroomData'\n",
    "annotDir = './BallroomAnnotations'\n",
    "# dataDir = './BallroomAuditor'\n",
    "\n",
    "# define reasonable limitations\n",
    "bpm_limits = [61, 200]\n",
    "tau_limits = []\n",
    "for bpm in bpm_limits:\n",
    "    freq = bpm / 60\n",
    "    tau = round(1 / (freq * hopSize_secs))\n",
    "    tau_limits.append(tau)\n",
    "\n",
    "# beat tracking\n",
    "analysis_fraction = 4 #analyses the first 1/n samples of the song to determine beat 1 location\n",
    "half_sqr_width = 5\n",
    "performance_deviation = 4\n",
    "tempo_dec_tol = .20 #percent\n",
    "tempo_inc_tol = .10\n",
    "alpha = 1\n",
    "final_offset = 0 #postprocessing\n",
    "\n",
    "if os.path.isdir('beats') == False:\n",
    "    os.makedirs('beats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_odf(filename, winSize_secs, hopSize_secs):\n",
    "#     print('file is: ', filename)\n",
    "    snd, rate = librosa.load(filename, sr=None)\n",
    "    hop = round(rate * hopSize_secs)\n",
    "    winLen_samps = rate * winSize_secs\n",
    "    # round up to next power of 2\n",
    "    wlen_samps = int(2 ** np.ceil(np.log2(winLen_samps)))\n",
    "#     print('window length is: ', wlen_samps)\n",
    "#     print('hop length is: ', hop)\n",
    "    # centre frames: first frame at t=0 - Kind of a hack way of thinking about it\n",
    "    snd = np.concatenate([np.zeros(wlen_samps//2), snd, np.zeros(wlen_samps//2)])\n",
    "    # how many frames will cover this sound recording - Do the '- wlen_samps' and  '+1' cancel each other out?\n",
    "    frameCount = int(np.floor((len(snd) - wlen_samps) / hop + 1))\n",
    "#     print('Frames per audio file', frameCount)\n",
    "    #odf is an object that holds 7 types of measurements for onsets\n",
    "    odf = np.zeros((7, frameCount))\n",
    "    ham_wind = np.hamming(wlen_samps)\n",
    "    #preempting prev values needed\n",
    "    prevM = np.zeros(wlen_samps)\n",
    "    prevA = np.zeros(wlen_samps)\n",
    "    prevprevA = np.zeros(wlen_samps)\n",
    "    # cycle through audio and add each calculation to the appropriate odf frame as you go\n",
    "    for i in range(frameCount):\n",
    "        start = i * hop\n",
    "        # mutiply signal frame by hamming frame and get fft of this\n",
    "        # remember that the fft consists of a real and imaginary number\n",
    "        currentFrame = np.fft.fft(snd[start: start+wlen_samps] * ham_wind)\n",
    "        \n",
    "        \"\"\"Root Mean Square\"\"\"\n",
    "        mags = np.abs(currentFrame)\n",
    "        squared_mags = np.power(mags, 2)\n",
    "        mean_sqr_mags = np.mean(squared_mags)\n",
    "        rms = np.sqrt(mean_sqr_mags)\n",
    "        odf[0][i] = rms\n",
    "#         print('rms', rms)\n",
    "        \n",
    "        \"\"\"High Freq Content\"\"\"\n",
    "        # Since mags is mirrored after the Nyquist index, multiplying this by a mirrored index and getting its\n",
    "        # mean will be the same as getting the mean of the first half of the mags \n",
    "        k_vector = list(range(wlen_samps//2)) + list(range(wlen_samps//2,0,-1))\n",
    "        hfc_calc1 = np.mean(np.multiply(squared_mags, k_vector))\n",
    "        odf[1][i] = hfc_calc1\n",
    "        \n",
    "        # hfc while trying to follow the algorithm (especially where k=-N/2)\n",
    "        weight_vector = np.arange(wlen_samps) - wlen_samps//2\n",
    "        # half the values cancel the other half out, except for 1, which is left\n",
    "        left_over = np.sum(np.multiply(weight_vector, squared_mags))\n",
    "        hfc_calc2 = 4/(wlen_samps*(wlen_samps+2)) * np.sum(np.multiply(weight_vector, squared_mags))\n",
    "        \n",
    "        \"\"\"Spectral FLuz\"\"\"       \n",
    "        sf = np.mean(np.multiply(np.subtract(mags, prevM),np.greater(mags, prevM)))\n",
    "#         print('sf',sf)\n",
    "        odf[2][i] = sf\n",
    "        \n",
    "        #apply half-wave retifier logic\n",
    "        h0 = ((mags - prevM) + abs(mags - prevM))/2\n",
    "        # when h0 element is less than 0, make it zero (using boolean array method)\n",
    "        H = h0[h0 > 0]\n",
    "        sf2 = np.sum(H) / wlen_samps\n",
    "        \n",
    "        \"\"\"Complex Domain\"\"\"\n",
    "        \n",
    "        phase = np.angle(currentFrame)\n",
    "        tPhase = np.subtract(np.multiply(prevA, 2), prevprevA)\n",
    "        cdVector = np.sqrt(np.subtract(np.add(np.power(prevM,2), np.power(mags,2)),\n",
    "                        np.multiply(np.multiply(np.multiply(prevM, mags), 2),\n",
    "                        # this is the princarg phase wrapping?\n",
    "                        np.cos(np.subtract(phase, tPhase)))))\n",
    "        odf[3][i] = np.mean(cdVector)\n",
    "        rcd = np.mean(np.multiply(np.greater_equal(mags, prevM), cdVector))\n",
    "        odf[4][i] = rcd\n",
    "        \n",
    "        \"\"\"Phase Deviation\"\"\"\n",
    "        pdVector = np.abs(np.divide(np.subtract(np.mod(np.add(\n",
    "                        np.subtract(phase, tPhase), np.pi), 2 * np.pi), np.pi), np.pi))\n",
    "        odf[5][i] = np.mean(pdVector)\n",
    "        if rms != 0:\n",
    "            wpd = np.divide(np.mean(np.multiply(pdVector, mags)), rms * 2)\n",
    "        else:\n",
    "            wpd=0\n",
    "        odf[6][i] = wpd\n",
    "        \n",
    "        rate_change_phase = phase - prevA\n",
    "        i_component=(phase + rate_change_phase)\n",
    "        target_complex_value = mags * np.exp(i_component)\n",
    "        prevM = mags\n",
    "        prevA = phase\n",
    "    return odf\n",
    "\n",
    "def normalize(arr):\n",
    "    mx = arr.max(axis=1)\n",
    "    for i in range(mx.shape[0]):\n",
    "        if mx[i] > 0:\n",
    "            arr[i,:] /= mx[i]\n",
    "    return arr\n",
    "\n",
    "def process_odf(selected_odf, maxFiltODF, threshold_array):\n",
    "    # convert odf to a signal that marks the instance and magnitude of the onset only\n",
    "    caught_odf = threshold_array[selected_odf]\n",
    "    odf = maxFiltODF[selected_odf]\n",
    "    threshold_odf = threshold[selected_odf]\n",
    "    type(threshold_odf), threshold_odf\n",
    "    caught_odf[caught_odf == threshold_odf[0]] = 0\n",
    "    # trim threshold values so there only a single peak present every 50ms (Dixon 2001)\n",
    "    onset_peak_indices, peak_value_dict = signal.find_peaks(caught_odf, height=0, distance=10)\n",
    "    peak_times = onset_peak_indices * hopSize_secs\n",
    "    peak_values = peak_value_dict['peak_heights']\n",
    "    trimmed_odf = np.zeros_like(caught_odf)\n",
    "    trimmed_odf[onset_peak_indices] = peak_values\n",
    "    return trimmed_odf, onset_peak_indices\n",
    "\n",
    "def acfTempo(trimmed_odf, tau_limits):\n",
    "    # autocorrelate the entire odf signal with itself\n",
    "    autocor = np.correlate(trimmed_odf, trimmed_odf, mode='full')[int(len(np.correlate(trimmed_odf, trimmed_odf, mode='full'))//2)+1:]\n",
    "    acf_peak_indices, acf_peak_value_dict = signal.find_peaks(autocor, height=np.max(autocor)/2, distance=5)\n",
    "    acf_peak_values = acf_peak_value_dict['peak_heights']\n",
    "    peaked_autocor = np.zeros_like(autocor)\n",
    "    peaked_autocor[acf_peak_indices] = acf_peak_values\n",
    "    best_tau_samples = np.argpartition(autocor, -4)[-4:]\n",
    "    peak_values = autocor[best_tau_samples]\n",
    "    #most reasonable value is between tau_limits\n",
    "    for best_tau_sample in best_tau_samples:\n",
    "        if best_tau_sample > tau_limits[1] and best_tau_sample < tau_limits[0]:\n",
    "            tactus_tau = best_tau_sample\n",
    "        else:\n",
    "            tactus_tau = np.min(best_tau_samples)\n",
    "    #  add 1 to tactus_tau (due to 0 indexing) to get the true sample delay\n",
    "    tactus_tau = tactus_tau + 1\n",
    "    return tactus_tau\n",
    "\n",
    "def beatTimes_to_array(trueOnsets, trimmed_odf):\n",
    "    # converts the trueOnsets to a usable array for visualisations\n",
    "    trueOnsets_resampled = trueOnsets * 100\n",
    "    trueOnsets_beats = np.zeros_like(trimmed_odf)\n",
    "    for beat_time in trueOnsets_resampled:\n",
    "        beat_time = int(beat_time)\n",
    "        trueOnsets_beats[beat_time] = 1\n",
    "    return trueOnsets_beats\n",
    "\n",
    "def find_first_beat(trimmed_odf, tactus_tau, onset_peak_indices):\n",
    "    # make a pulse train\n",
    "    window_length = len(trimmed_odf)//analysis_fraction\n",
    "    # THIS GIVES YOU A VISUALISATION OF HOW THEY MATCH UP\n",
    "    # Larcohe 2003, Alonos 2004\n",
    "    # -5 to 5 used to represent precentage that tempo can vary in either direction\n",
    "    sqr_pulse = np.zeros_like(trimmed_odf)\n",
    "    for i in range(len(trimmed_odf)//tactus_tau):\n",
    "        index = i * tactus_tau\n",
    "        for i in range(-half_sqr_width,half_sqr_width):\n",
    "            sqr_pulse[index+i] = 1\n",
    "\n",
    "    correlation_at_onsets = []\n",
    "    # assume the first 10 onsets - one of them is the downbeat\n",
    "    initial_onsets_to_analyse = 10\n",
    "    # just empircally testing to scan first 6 onsets and choose the best one that will tell me where the first onset is\n",
    "    for onset_peak_index in onset_peak_indices[:initial_onsets_to_analyse]:\n",
    "        trimmed_odf_window = trimmed_odf[onset_peak_index : onset_peak_index+window_length]\n",
    "        if len(trimmed_odf_window) < window_length:\n",
    "            break\n",
    "        pulse_window = sqr_pulse[:window_length]\n",
    "        summed_sigs = np.sum(trimmed_odf_window * pulse_window)\n",
    "        correlation_at_onsets.append(summed_sigs)\n",
    "    # plt.plot(correlation_at_onsets)\n",
    "    onset_peak_index = np.where(np.asarray(correlation_at_onsets) == np.max(np.asarray(correlation_at_onsets)))[0][0]\n",
    "    first_beat_index = onset_peak_indices[onset_peak_index]\n",
    "    return first_beat_index\n",
    "\n",
    "def find_beats(trimmed_odf, tactus_tau, first_beat_index):\n",
    "    beat_indices = [first_beat_index]\n",
    "\n",
    "    tactus_dec_tol = round(tactus_tau * tempo_dec_tol) #tempo increase means increasing tau value\n",
    "    tactus_inc_tol = round(tactus_tau * tempo_inc_tol) #tempo increase means decreasing tau value\n",
    "    outside_perf_region = 0\n",
    "    missing_onsets = 0\n",
    "    within_tempo_region = 0\n",
    "\n",
    "    while beat_indices[-1] < len(trimmed_odf)-tactus_tau:\n",
    "        next_beat_pred = beat_indices[-1] + tactus_tau\n",
    "    #     print('next_beat_pred',next_beat_pred)\n",
    "        performance_region = trimmed_odf[next_beat_pred - performance_deviation: next_beat_pred + performance_deviation+1]\n",
    "        onset_locations = np.where(performance_region != 0)[0]\n",
    "        if len(onset_locations)>0:\n",
    "            missing_onsets = 0\n",
    "            # find out which location is closer to 5\n",
    "            closest_to_centre = performance_deviation + 2\n",
    "            for onset in onset_locations:\n",
    "                if abs(onset - performance_deviation) < closest_to_centre:\n",
    "                    closest_index = onset\n",
    "                    closest_to_centre = abs(onset - performance_deviation)\n",
    "            offset = closest_index - performance_deviation\n",
    "            adjusted_beat_pred = next_beat_pred + offset\n",
    "        else:\n",
    "            # check the window tollerance for tempo change\n",
    "            tempo_change_region = trimmed_odf[next_beat_pred - tactus_inc_tol: next_beat_pred + tactus_dec_tol+1] \n",
    "            onset_locations = np.where(tempo_change_region != 0)[0]\n",
    "            if len(onset_locations)>0:\n",
    "                missing_onsets = 0\n",
    "                closest_to_centre = tactus_dec_tol + 2 # just to start off\n",
    "                for onset in onset_locations:\n",
    "                    centered_index = tactus_dec_tol-tactus_inc_tol\n",
    "                    if abs(centered_index - onset) < closest_to_centre:\n",
    "                        closest_index = onset\n",
    "                        closest_to_centre = abs(centered_index-onset)\n",
    "    #             pdb.set_trace()\n",
    "                offset = closest_index - centered_index\n",
    "                adjusted_beat_pred = next_beat_pred + offset\n",
    "                tactus_tau = round((alpha*(tactus_tau + offset)) + ((1 - alpha)*tactus_tau))\n",
    "            else:\n",
    "                # assign a beat to default place\n",
    "                missing_onsets += 1\n",
    "                adjusted_beat_pred = next_beat_pred\n",
    "    #     print('adjusted_beat_pred',adjusted_beat_pred)\n",
    "        beat_indices.append(adjusted_beat_pred)\n",
    "\n",
    "\n",
    "    # if last onsets were missed, remove them\n",
    "    beat_indices = beat_indices[:len(beat_indices)-missing_onsets]\n",
    "\n",
    "    missing_onsets = 0\n",
    "    no_more_anacrusis = False\n",
    "    while beat_indices[0] - tactus_tau > 0 and no_more_anacrusis == False:\n",
    "    #     pdb.set_trace()\n",
    "        previous_beat_pred = beat_indices[0] - tactus_tau\n",
    "        anacrusis_region = trimmed_odf[previous_beat_pred-performance_deviation: previous_beat_pred+performance_deviation]\n",
    "        onset_locations = np.where(anacrusis_region != 0)[0]\n",
    "        if len(onset_locations)>0:\n",
    "            # find out which location is closer to 5\n",
    "            closest_to_centre = performance_deviation + 2\n",
    "            for onset in onset_locations:\n",
    "                if abs(onset - performance_deviation) < closest_to_centre:\n",
    "                    closest_index = onset\n",
    "                    closest_to_centre = abs(onset - performance_deviation)\n",
    "            offset = closest_index-performance_deviation\n",
    "    #         pdb.set_trace()\n",
    "            adjusted_beat_pred = previous_beat_pred + offset\n",
    "            beat_indices.insert(0, adjusted_beat_pred)\n",
    "        else:\n",
    "            no_more_anacrusis = True\n",
    "    beat_indices = beat_indices[missing_onsets:]\n",
    "    return beat_indices\n",
    "\n",
    "def beatTracker(input_file='Albums-Cafe_Paradiso-14.wav'):\n",
    "    f_path = './' +input_file\n",
    "    odf = get_all_odf(f_path, winSize_secs, hopSize_secs)\n",
    "    # FILTERING AND PROCESSING OF THE ODFs\n",
    "    medFiltODF = odf - medfilt(odf, wd)\n",
    "    for row in medFiltODF:\n",
    "        maxFiltEntry = maxfilt(medFiltODF, wd, mode='nearest', axis=0)\n",
    "        try:\n",
    "            np.concatenate((maxFiltODF, maxFiltEntry), axis=0)\n",
    "        except:\n",
    "            maxFiltODF = maxFiltEntry\n",
    "    maxFiltODF = maxfilt(medFiltODF, wd, mode='nearest', axis=1)\n",
    "    maxFiltODF = np.gradient(maxFiltODF, axis=1)\n",
    "    maxFiltODF = np.divide(np.subtract(maxFiltODF, np.mean(maxFiltODF)), np.std(maxFiltODF))\n",
    "    maxFiltODF = normalize(maxFiltODF)\n",
    "    # removing any artefacts causing false positives at the very start or end - this would not interefere with any beates\n",
    "    maxFiltODF[:,:3] = maxFiltODF[:,3000:] = 0\n",
    "    # FILL THRESHOLD ARRAY WITH VALUES THAT EXCEED EACH THRESHOLF\n",
    "    threshold_array = np.ones(maxFiltODF.shape) * threshold\n",
    "    for row_idx in range(len(maxFiltODF)):\n",
    "        for col_idx in range(len(maxFiltODF[row_idx])):\n",
    "            threshold_array[row_idx][col_idx] = max(maxFiltODF[row_idx][col_idx], threshold[row_idx]) \n",
    "    # PROCESS ODF\n",
    "    trimmed_odf, onset_peak_indices = process_odf(selected_odf, maxFiltODF, threshold_array)\n",
    "    # AUTOCORRELATION TO DETERMINE TEMPO\n",
    "    tactus_tau = acfTempo(trimmed_odf, tau_limits)\n",
    "    first_beat_index = find_first_beat(trimmed_odf, tactus_tau, onset_peak_indices)\n",
    "    beat_indices = find_beats(trimmed_odf, tactus_tau, first_beat_index)\n",
    "    beat_indices = np.asarray(beat_indices) - final_offset\n",
    "    beat_times = beat_indices * hopSize_secs\n",
    "    return beat_times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single line Implementation of beatTracker\n",
    "Insert filepath in parantheses for custom analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2 ,  1.91,  2.61,  3.32,  4.02,  4.71,  5.4 ,  6.1 ,  6.79,\n",
       "        7.51,  8.19,  8.9 ,  9.61, 10.28, 10.98, 11.68, 12.39, 13.1 ,\n",
       "       13.79, 14.49, 15.17, 15.88, 16.58, 17.28, 17.96, 18.67, 19.37,\n",
       "       20.08, 20.78, 21.48, 22.18, 22.88, 23.57, 24.28, 25.02, 25.69,\n",
       "       26.36, 27.07])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beats = beatTracker()\n",
    "beats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvMI",
   "language": "python",
   "name": "venvmi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
